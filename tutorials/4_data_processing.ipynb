{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# ARPES data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import peaks as pks\n",
    "import os\n",
    "import pint_xarray\n",
    "\n",
    "# Example data\n",
    "from peaks.core.utils.sample_data import ExampleData, plot_tutorial_example_figure\n",
    "\n",
    "# Set default options\n",
    "xr.set_options(cmap_sequential='Purples', keep_attrs=True)\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "ureg = pint_xarray.unit_registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binding energy and k-space conversions  \n",
    "The function :class:`peaks.core.processing.k_convert`, most conveniently accessed via the accessor `.k_convert`, allows converting ARPES data to binding energy and $k$-space, with conventions automatically following the analyser type and taking care of different scan types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to binding energy\n",
    "To convert data to binding energy, we need some reference for the Fermi level. This should be stored as a metadata entry under the `calibration` group, and usually set via the metadata method `.set_EF_correction()`. \n",
    "\n",
    ":::{note}Photon energy scans\n",
    "For most scans, a single `EF_correction` attribute is maintained. However, for [photon energy scans](#photon-energy-scans), an additional $hv$-dependent correction is added as another data coordinate. \n",
    ":::\n",
    "\n",
    "The following references of the Fermi level correction are currently accepted:\n",
    "- fitting parameters generated by a fit to the Fermi energy of e.g. a poly gold sample, in a dictionary form of polynomial coefficients, e.g.:\n",
    "```\n",
    "{'c0': 105.07837338559817,\n",
    " 'c1': 0.00021092006222556432,\n",
    " 'c2': -6.373415730141268e-05,\n",
    " 'c3': -2.3182740086084596e-07}\n",
    "```\n",
    "- The correct form is automatically output by fitting a reference sample using `peaks`'s in-built [Au fitting function](./5_data_fitting.ipynb#fitting-au-fermi-edge-data), which can be passed directly to `.metadata.set_EF_correction()`\n",
    "- a float, resulting in a rigid shift of the Fermi level.\n",
    "\n",
    "E.g. taking a reference poly gold scan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load relevant Au correction\n",
    "gold = ExampleData.gold_reference2().sel(eV=slice(16.5,None), theta_par=slice(-10,10)).bin_data(theta_par=4)\n",
    "gold_fit_result = gold.fit_gold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting fit is stored as an attribute of the returned fit result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "gold_fit_result.EF_correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $E_\\mathrm{F}$ correction can be set directly from the fit_result using the metadata `set_EF_correction` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold.metadata.set_EF_correction(gold_fit_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underlying data structure defining the EF correction can be obtained via the `.get_EF_correction()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold.metadata.get_EF_correction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EF correction can also be defined to be like another scan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dispersion\n",
    "disp1 = ExampleData.dispersion2a()\n",
    "\n",
    "# Set the Fermi correction reference from the gold scan which already has this applied\n",
    "disp1.metadata.set_EF_correction_like(gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp1.metadata.calibration.EF_correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also be passed directly to set all members of a :class:`xarray.DataTree`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into a tree\n",
    "dt = xr.DataTree()\n",
    "dt.add(ExampleData.gold_reference2())\n",
    "dt.add(ExampleData.dispersion2a())\n",
    "\n",
    "# Set the EF_correction\n",
    "dt.metadata.set_EF_correction_like(gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.Gold.data.metadata.calibration.EF_correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-conversion\n",
    "`peaks` implements momentum conversion assuming the use of a hemispherical-type analyser with an entrance slice, with or without deflectors for virtual scanning of the angular axes, following the conventions of Ishida and Shin (https://aip.scitation.org/doi/10.1063/1.5007226). See the [documentation](https://research.st-andrews.ac.uk/kinggroup/peaks/latest/explanations/angle_conventions.html) for more information and a discussion of convention choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dispersions\n",
    "To process e.g. a dispersion: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate normal emission from our dispersion\n",
    "disp1.plot()\n",
    "plt.axvline(3.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the normal emission values as described in the [metadata guide](./1_getting_started.ipynb#setting_manipulator_normal_emissions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp1.metadata.set_normal_emission(theta_par=3.85*ureg.deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp1.metadata.manipulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp1_k = disp1.k_convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp1_k.plot()\n",
    "plt.axvline(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify a desired k-point range and step size by providing the `ky` argument (note depending on the analyser, this could be `kx`) as a `slice(start, stop, step)`. Note, any of these can be retained at the default by passing `None` for that parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp1_k2 = disp1.k_convert(ky=slice(-0.4,None,0.05))\n",
    "pks.plot_grid([disp1_k, disp1_k2], titles=[\"Auto\", \"$\\\\Delta{}k=0.05\\\\;\\\\AA^{-1}$\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "Specifying a reduced step size for the $k$ conversion only reduces the sampling density for the interpolation, it does not bin the data. If you are happy with a larger step size, it is better to first [bin the data](#data-binning) using e.g.:\n",
    "\n",
    "```python\n",
    "disp1.bin_data(theta_par=2)\n",
    "```\n",
    ":::\n",
    "\n",
    "Note that the methods attempt to estimate unspecified parameters if they are not set - note the `Analysis warning` boxes returned. E.g. if passing a freshly loaded dispersion without setting the `EF_correction` attribute, an automatic determination of the Fermi level will be attempted: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp1_copy = ExampleData.dispersion2a()\n",
    "disp1_copy.k_convert().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{warning}\n",
    "These automatic estimations are useful for a quick look, but should not be relied upon for careful analysis!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fermi surfaces\n",
    "Fermi surface corrections work in the same way as for dispersions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load a Fermi map and relevant Au correction\n",
    "FS = ExampleData.FS()\n",
    "gold = ExampleData.gold_reference()\n",
    "# Select a smaller range\n",
    "gold = gold.sel(theta_par=slice(-16,17), eV=slice(104.9,105.2)).bin_data(theta_par=5,eV=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine normal emission in FS.disp()\n",
    "FS.metadata.set_normal_emission({'polar': '1.499 deg', 'tilt': '0.020999999999999998 deg', 'azi': '-12 deg'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FS.metadata.manipulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result = gold.fit_gold()\n",
    "FS.metadata.set_EF_correction(fit_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To return just a single slice in energy, the `eV_slice=(centre, width)` argument can be used, with the arguments specified in binding energy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FSM_k = FS.k_convert(eV_slice=(-0.05,0.02))\n",
    "FSM_k.plot()\n",
    "plt.axvline(0)\n",
    "plt.axhline(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the full cube can be $k$-converted\n",
    "\n",
    ":::{tip}\n",
    "`peaks` uses :class:`numba` and :class:`numexpr` to accelerate the interpolations and relevant angle <--> $k$ calculations, making the $k$-conversions rather fast. Still, for a large map these can still be somewhat time consuming, and it is often better perform binning on the data first.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FS_bin = FS.bin_spectra(2)  # 2x2 bin on the eV and theta_par dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FS_bin_k = FS_bin.k_convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(10,4))\n",
    "FS_bin_k.sel(kx=0,method='nearest').plot(y='eV', ax=axes[0])\n",
    "FS_bin_k.sel(ky=0,method='nearest').plot(y='eV', ax=axes[1])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Photon energy scans\n",
    "\n",
    "#### Loading hv-dep data/data format\n",
    "In an $h\\nu$ scan, the most natural representation of the data is as a 3d cube of ($hv$,theta_par,eV), however the eV axis in raw kinetic energy changes as a function of photon energy, adding some extra complexity.\n",
    "\n",
    "In :class:`peaks`, the convention is to load (or assemble) a $h\\nu$-dependent dataset into such a cube, where the eV coordinate (when in kinetic energy mode) is that of the first scan, while a second non-dimension coordinate is included which shows the relevant offset of the KE scale as a function of hv. A warning on load indicates this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (kz map)\n",
    "hv1 = ExampleData.hv_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "Due to this perculiar data structure, if a single scan is selected using the usual `.sel(hv=...)` method, the incorrect energy scale will be returned. We have a helper method, `disp_from_hv` to overcome this problem\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pks.plot_grid([hv1.sel(hv=70, method='nearest'),hv1.disp_from_hv(70)], titles=['from `sel`', 'from `disp_from_hv`'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the incorrect energy scale on the left!\n",
    "\n",
    "#### Calibration\n",
    "##### Normal emission\n",
    "The normal emission is set as normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and set normal emission\n",
    "hv1.disp_from_hv(70).sel(theta_par=slice(-4,5)).plot()\n",
    "plt.axvline(1.05)\n",
    "hv1.metadata.set_normal_emission(theta_par=1.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inner potential\n",
    "An inner potential should be set in `.metadata.calibration.V0`. If nothing is set, a default value of 12 eV is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv1.metadata.calibration.V0 = 15 * pks.ureg('eV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EF_correction\n",
    "For the energy calibration, a regular `EF_correction` can be set, but also a $h\\nu$-dependent shift must be included as a co-ordinate within the :class:`xarray.DataArray` itself.\n",
    "\n",
    "We set the regular angle-dependent part like normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a gold reference scan for a single energy\n",
    "gold = ExampleData.gold_reference3().sel(theta_par=slice(-15,15),eV=slice(22.2,22.7))\n",
    "fit_result = gold.fit_gold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the angle-dep part\n",
    "hv1.metadata.set_EF_correction(fit_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $h\\nu$-dependent correction can be added as a new co-ordinate:\n",
    "\n",
    "```python\n",
    "data = data.coords.update({\"EF\": (\"hv\", EF_values)})\n",
    "```\n",
    "where `EF_values` is a :class:`numpy.ndarray` of length equal to the number of photon energies. Or it can be automatically estimated from `.estimate_EF()` which is run automatically if the `.k_convert` function is run with no Fermi correction already set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $k$-conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do the k-conversion and the rest is estimated\n",
    "hv1_k = hv1.k_convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a slice through the map\n",
    "hv1_k.drop_zero_borders().MDC(-0.40, 0.02).plot()\n",
    "plt.axvline(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "Note the `.drop_zero_borders()` method (:class:`peaks.core.process.data_select.drop_zero_borders`)in the above - this trims any rows/columns/slices that are only zeros, useful to trim the borders. :class:`peaks.core.process.data_select.drop_nan_borders` is an equivalent function for border regions that are full of only NaNs.\n",
    ":::\n",
    "\n",
    "The other arguments for selecting the $k$ and $E$ range returned work as before, while an additional argument `return_kv_scan_in_hv` allows for the data to be returned vs. photon energy instead of $k_z$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv1_kip = hv1.k_convert(return_kz_scan_in_hv=True)\n",
    "hv1_kip.MDC(-0.40, 0.02).drop_zero_borders().plot()\n",
    "plt.axvline(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging data\n",
    "Scans can be merged together using the :class:`peaks.core.process.tools.merge_data` function, avaialble as `pks.merge_data`. By default this acts along the `theta_par` dimension, but can be applied along any dimension and to higher dimensional data, with various in-built options for specifying the offsets and pre-cropping the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load multiple scans\n",
    "disps = xr.DataTree() \n",
    "for i, scan in enumerate(['a','b','c']):\n",
    "    disps.add(getattr(ExampleData,f\"dispersion2{scan}\")(), name=f'Scan {i+1}')\n",
    "\n",
    "# Merge the data, applying offsets in the default angle direction and trimming the data along this dimension \n",
    "merged_disp = pks.merge_data(disps, offsets=12, sel=slice(-10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disps.add(merged_disp, name='Merged scan')\n",
    "disps.plot_grid(ncols=4, sharey=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation\n",
    "To normalise data by a float, EDC/MDC, or region of interest, use :class:`peaks.core.process.tools.norm`, available via the `.norm()` accessor. This broadcasts across any number of dimenisons. Call with no argument to normalise to the max value, normalise by the mean by passing `all`, or by an integrated DC along a specified dimension, `dim`, or pass a keyword argument with `dim=slice(start,stop)` to define the region to make a DC to align by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp1_norm1 = disps['Scan 1'].data.norm()\n",
    "disp1_norm2 = disps['Scan 1'].data.norm(eV=slice(17,17.5))\n",
    "disp1_norm3 = disps['Scan 1'].data.norm('eV')\n",
    "pks.plot_grid([disps['Scan 1'].data, disp1_norm1, disp1_norm2, disp1_norm3],\n",
    "              titles=['Original','Normalised to max', \"Normalised by MDC above $E_F$\", \"Normalised by integrated MDC\"],\n",
    "             ncols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background subtraction\n",
    "Background subtraction works in exactly the same way, except calling `.bgs`, with the relevant parameters passed definie the data to subtract, except that either the `subtraction` arguement must be passed (can be a `float` or `int` to subtract a constant), or a keyword argument must be supplied specifying the background region to subtract. In addition `subtraction=\"Shirley\"` can be passed to specify subtraction of a Shirley background, in which case some additional arguments can be supplied to define options for the Shirley background determination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp1_bgs1 = disps['Scan 1'].data.bgs(5*pks.ureg('kcount/s'))\n",
    "disp1_bgs2 = disps['Scan 1'].data.bgs(5000)\n",
    "disp1_bgs3 = disps['Scan 1'].data.bgs(eV=slice(17,17.5))\n",
    "disp1_bgs4 = disps['Scan 1'].data.bgs('theta_par')\n",
    "disp1_bgs5 = disps['Scan 1'].data.bgs('Shirley')\n",
    "pks.plot_grid([disps['Scan 1'].data, disp1_bgs1, disp1_bgs2, disp1_bgs3, disp1_bgs4, disp1_bgs5],\n",
    "              titles=['Original','Constant bg of 5 kcount/s subtracted', 'Constant bg of 5000 subtracted', 'Subtracted MDC above $E_F$', \"Subtracted integrated EDC\", \"Subtracted Shirley BG\"],\n",
    "             ncols=3,\n",
    "             vmin=0,\n",
    "             sharey=True,\n",
    "             add_colorbar=False,\n",
    "             figsize=(14,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data binning\n",
    "Binning can be performed using the `.bin_data()` function, which provides a thin wrapper around the built-in :class:`xarray.coarsen` function, but with updating of analysis history built in, and with a shortcut for uniform binning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = ExampleData.dispersion()\n",
    "binned_disps = [disp,\n",
    "         disp.bin_data(2),\n",
    "         disp.bin_data(eV=8,theta_par=10, boundary='pad'),\n",
    "        ]\n",
    "pks.plot_grid(binned_disps,\n",
    "              titles=['Original', '2x2 binning applied', '8x10 binning applied with boundary padding'],\n",
    "             sharey=True,\n",
    "             add_colorbar=False\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.bin_spectra` can also be used to enforce binning on only the spectral dimensions of an ARPES detector (`eV`, `theta_par`, `k_par`), useful e.g. for binning of spatial mapping or Fermi surface data to avoid binning also the spatial or coarse angular dimentions. Both can be applied directly to :class:`xarray.DataTree`s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disps.bin_spectra(8).plot_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing\n",
    "\n",
    "Smoothing of data can be applied using `peaks.core.process.tools.smooth`, or via the `.smooth` accessor, passing keyword argument(s) in the format `axis=FWHM`, where FWHM is the relevant FWHM of the Gaussian for convolution in this direction, e.g. eV=0.1. This broadcasts to multidimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_smooth = disp.smooth(eV=10*pks.ureg('meV'),theta_par=1)\n",
    "pks.plot_grid([disp, disp_smooth],\n",
    "              titles=['Original','Smoothed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative-type methods\n",
    "We have various wrappers around the built-in :class:`xarray.DataArray.differentiate` method to perform derivative methods for ARPES analysis, including updating the analysis history. `.deriv(dim=list_of_dims)` is our general method, but specific `.d2E()`, `.d2k()`, `.dEdk()`, `.dkdE()`, `.curvature()` and `.min_gradient()` methods exist to ease use of derivative methods for ARPES analsys. Note, often data must be pre-smoothed. \n",
    "\n",
    ":::{warning}\n",
    "Note, derivative methods can introduce significant artefacts in ARPES data - use with extreme caution!\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_d2k = disp.sel(theta_par=slice(-15,15)).smooth(eV=0.01,theta_par=0.2).d2k()\n",
    "disp_curv = disp.sel(theta_par=slice(-15,15)).smooth(eV=0.03,theta_par=0.5).curvature(theta_par=500, eV=50)\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(12,4))\n",
    "disp.sel(theta_par=slice(-15,15)).plot(ax=ax[0])\n",
    "ax[0].set_title('Original')\n",
    "disp_d2k.plot(ax=ax[1], vmax=0)\n",
    "ax[0].set_title('d2/dk^2')\n",
    "disp_curv.plot(ax=ax[2], robust=True)\n",
    "ax[0].set_title('Curvature')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symmetrisation\n",
    "The :class:`peaks.core.process.tools.sym` function (`.sym()` accessor) allows to symmetrise (or alternatively just flip) data around a given axis defined via a keyword argument to the function call.\n",
    "\n",
    "### $E_\\mathrm{F}$ symmetrisation\n",
    "If no arguments are supplied, `.sym()` defults to symmetrising in energy around `eV=0`, useful e.g. for symmetrising about the Fermi level for gap analysis etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp1_k.sym().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp1.sym(eV=16.8*pks.ureg('eV'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disps.sym(eV=16.8).plot_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This broadcasts to higher dimensions, and can be used to also symmetrise in a different dimension, but can only be applied to symmetrisiation in a single axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotational symmetrisation\n",
    "The :class:`peaks.core.process.tools.sym_nfold` function (`.sym_nfold()` accessor) allows for $n$-fold symmetrisation of a 2D :class:`xarray.DataArray` around specified centre co-ordinates (defaults to (0,0))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FSM_k_s3 = FSM_k.sym_nfold(3, kx=0.0, ky=0)\n",
    "pks.plot_grid([FSM_k,FSM_k_s3], titles=['Original','3-fold symmetrised'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid removal\n",
    "\n",
    ":::{note}\n",
    "`peaks` currently has only a basic implementation of analyser detector mesh removal. For a more complete implementation see [Liu et al.](https://www.sciencedirect.com/science/article/pii/S0368204822000883)\n",
    ":::\n",
    "\n",
    "Automated mesh removal can be attempted with the `.degrid()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some data with detector mesh visible\n",
    "disp2 = ExampleData.dispersion3()\n",
    "disp2_dg = disp2.degrid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{warning}\n",
    "This feature is currently experimental, and as is clear in the above example, can introduce artefacts in the data. Use with care!\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peaks-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
